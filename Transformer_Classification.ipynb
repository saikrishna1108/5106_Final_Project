{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1c0cae1-ab66-4880-bb5e-02a75b7f479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "class SpeedDataset(Dataset):\n",
    "    def __init__(self, directory, sequence_length=5):\n",
    "        self.data = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_speed = 0  # Variable to store the maximum speed found\n",
    "        self.preprocess_data(directory)\n",
    "        # Calculate number of classes based on the range from 30 to 105\n",
    "        self.num_classes = (105 - 30) // 10 + 1  # From class 30-39, ..., 100-105\n",
    "\n",
    "    def preprocess_data(self, directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                speed = float(filename.split('_')[-1].replace('.txt', ''))\n",
    "                # Update maximum speed if current speed is greater\n",
    "                if speed > self.max_speed:\n",
    "                    self.max_speed = speed\n",
    "                # Convert speed to class, starting range from 30\n",
    "                if speed < 30:\n",
    "                    speed_class = 0  # Handle speeds below 30 as class 0\n",
    "                else:\n",
    "                    speed_class = (int(speed) - 30) // 10\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, 'r') as file:\n",
    "                    track_data = {}\n",
    "                    for line in file:\n",
    "                        frame, track_id, x1, y1, x2, y2 = map(float, line.strip().split(',')[:6])\n",
    "                        if track_id not in track_data:\n",
    "                            track_data[track_id] = []\n",
    "                        track_data[track_id].append([x1,y1,x2,y2,(x1+x2)/2, (y1+y2)/2])\n",
    "\n",
    "                    for track_id, frames in track_data.items():\n",
    "                        if len(frames) >= self.sequence_length:\n",
    "                            features = []\n",
    "                            for i in range(1, len(frames)):\n",
    "                                current_frame = frames[i]\n",
    "                                previous_frame = frames[i-1]\n",
    "                                data_d = []\n",
    "                                for i in range(6):\n",
    "                                    data_d.append(current_frame[i])\n",
    "                                for j in range(4,6):\n",
    "                                    data_d.append(current_frame[j] - previous_frame[j])\n",
    "                                features.append(data_d)\n",
    "                                \n",
    "                            for i in range(len(features) - self.sequence_length + 1):\n",
    "                                sequence = features[i:i + self.sequence_length]\n",
    "                                self.data.append((sequence, speed_class))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs, output = self.data[idx]\n",
    "        return torch.tensor(inputs, dtype=torch.float32), torch.tensor(output, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d76b3a61-4a95-4862-89a7-226fee005802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Move the positional encoding to the same device as the input tensor\n",
    "        encoding = self.encoding[:, :x.size(1)].to(x.device)\n",
    "        return x + encoding.detach()\n",
    "\n",
    "\n",
    "class SpeedPredictor(nn.Module):\n",
    "    def __init__(self, sequence_length, feature_size, embedding_dim, hidden_dim, num_classes, n_heads=2, n_layers=2):\n",
    "        super(SpeedPredictor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(feature_size, embedding_dim, kernel_size=1)\n",
    "        self.fc2 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim,num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=n_heads,\n",
    "            batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers=n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pos_encoder(x)  # Apply positional encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, -1, :]  # Get the output from the last timestep\n",
    "        x = self.fc2(x)\n",
    "        return self.relu(self.fc1(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4bc4575-cc41-4d0d-91b0-1b8192ae5516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8604b79-ccfc-44b0-abc2-2b840e5fac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Ensure inputs and targets are on the same device as model\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_train += (predicted == targets).sum().item()\n",
    "            total_train += targets.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            model.eval()\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in test_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)  # Ensure inputs and targets are on the same device as model\n",
    "                    outputs = model(inputs)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct_test += (predicted == targets).sum().item()\n",
    "                    total_test += targets.size(0)\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_targets.extend(targets.cpu().numpy())\n",
    "            test_accuracy = 100 * correct_test / total_test\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {total_loss / len(train_loader)}, '\n",
    "                  f'Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4ecd66e3-12ed-4676-b6bb-d376d7af626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.079562432584707, Train Accuracy: 18.36%, Test Accuracy: 18.88%\n",
      "Epoch 11: Train Loss: 2.0794413089752197, Train Accuracy: 18.44%, Test Accuracy: 18.88%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Train and Evaluate\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     15\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = SpeedDataset('outputs', sequence_length=5)\n",
    "test_dataset = SpeedDataset('test', sequence_length=5)\n",
    "max_classes = max(train_dataset.num_classes, test_dataset.num_classes)\n",
    "model = SpeedPredictor(sequence_length=10, feature_size=8, embedding_dim=64, hidden_dim=32, num_classes=max_classes,n_heads =2 , n_layers = 2)\n",
    "model.to(device)  # Move model to the appropriate device\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Move criterion to the appropriate device\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "# Train and Evaluate\n",
    "train(model, train_loader, test_loader, criterion, optimizer, epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72b22e-8a5f-40e0-8560-c9e63937220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix_and_report(all_targets, all_preds):\n",
    "    print(confusion_matrix(all_targets, all_preds))\n",
    "    print(classification_report(all_targets, all_preds, target_names=[f'Class {30 + i * 10}-{39 + i * 10}' for i in range(max(all_targets) + 1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a1c0d-8662-47f9-9c4d-3ef9e56e0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Ensure inputs and targets are on the same device as model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_test += (predicted == targets).sum().item()\n",
    "            total_test += targets.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    print(\"Confusion Matrix and Classification Report:\")\n",
    "    print_confusion_matrix_and_report(all_targets, all_preds)\n",
    "    return test_accuracy\n",
    "evaluate(model, test_loader)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
