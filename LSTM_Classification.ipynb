{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450b68b7-329c-4a25-92cf-9dd033b1e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import math\n",
    "class SpeedDataset(Dataset):\n",
    "    def __init__(self, directory, sequence_length=5):\n",
    "        self.data = []\n",
    "        self.sequence_length = sequence_length\n",
    "        self.max_speed = 0  # Variable to store the maximum speed found\n",
    "        self.preprocess_data(directory)\n",
    "        # Calculate number of classes based on the range from 30 to 105\n",
    "        self.num_classes = (105 - 30) // 10 +1   # From class 30-39, ..., 100-105\n",
    "\n",
    "    def preprocess_data(self, directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                speed = float(filename.split('_')[-1].replace('.txt', ''))\n",
    "                # Update maximum speed if current speed is greater\n",
    "                if speed > self.max_speed:\n",
    "                    self.max_speed = speed\n",
    "                # Convert speed to class, starting range from 30\n",
    "                if speed < 30:\n",
    "                    speed_class = 0  # Handle speeds below 30 as class 0\n",
    "                else:\n",
    "                    speed_class = (int(speed) - 30) // 10\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, 'r') as file:\n",
    "                    track_data = {}\n",
    "                    for line in file:\n",
    "                        frame, track_id, x1, y1, x2, y2 = map(float, line.strip().split(',')[:6])\n",
    "                        if track_id not in track_data:\n",
    "                            track_data[track_id] = []\n",
    "                        track_data[track_id].append([x1,y1,x2,y2,(x1+x2)/2, (y1+y2)/2])\n",
    "\n",
    "                    for track_id, frames in track_data.items():\n",
    "                        if len(frames) >= self.sequence_length:\n",
    "                            features = []\n",
    "                            for i in range(1, len(frames)):\n",
    "                                current_frame = frames[i]\n",
    "                                previous_frame = frames[i-1]\n",
    "                                data_d = []\n",
    "                                distance = 0\n",
    "                                distance1 = 0\n",
    "                                distance2 = 0\n",
    "                                for i in range(4):\n",
    "                                    data_d.append(abs(current_frame[i] - previous_frame[i]))\n",
    "                                for j in range(4,6):\n",
    "                                    distance = ((current_frame[j] - previous_frame[j])**2) + distance\n",
    "                                for j in range(0,2):\n",
    "                                    distance1 = ((current_frame[j] - previous_frame[j])**2) + distance1\n",
    "                                for j in range(2,4):\n",
    "                                    distance2 = ((current_frame[j] - previous_frame[j])**2) + distance2\n",
    "                                data_d.append(math.sqrt(distance))\n",
    "                                data_d.append(math.sqrt(distance1))\n",
    "                                data_d.append(math.sqrt(distance2))\n",
    "                                features.append(data_d)\n",
    "                            for i in range(len(features) - self.sequence_length + 1):\n",
    "                                sequence = features[i:i + self.sequence_length]\n",
    "                                self.data.append((sequence, speed_class))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs, output = self.data[idx]\n",
    "        return torch.tensor(inputs, dtype=torch.float32), torch.tensor(output, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8245562-d2e4-4b96-b02d-1f8e325d625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpeedPredictor(nn.Module):\n",
    "    def __init__(self, sequence_length, feature_size, embedding_dim, hidden_dim, output_size):\n",
    "        super(SpeedPredictor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_size, out_channels=embedding_dim, kernel_size=1)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, num_layers = 3)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=hidden_dim, out_channels=64, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Use only the last output of LSTM for further processing\n",
    "        x = x.unsqueeze(2)  # Adjust dimensions for the second conv layer\n",
    "        x = self.conv2(x)\n",
    "        x = x.squeeze(2)  # Flatten the output for FC layers\n",
    "        return self.relu(self.fc2(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa6932f-cadb-40ca-9f17-a69866cca6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e9374e2-b8f8-4c42-bf2b-7b02698f7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        Train_total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Ensure inputs and targets are on the same device as model\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # First, update the parameters with the current learning rate\n",
    "            \n",
    "            Train_total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_train += (predicted == targets).sum().item()\n",
    "            total_train += targets.size(0)\n",
    "        \n",
    "        scheduler.step()  # After optimizer updates, adjust the learning rate\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            model.eval()\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            total_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in test_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    total_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct_test += (predicted == targets).sum().item()\n",
    "                    total_test += targets.size(0)\n",
    "                    \n",
    "            train_accuracy = 100 * correct_train / total_train\n",
    "            test_accuracy = 100 * correct_test / total_test\n",
    "            print(f'Epoch {epoch+1}: Train Loss: {Train_total_loss / len(train_loader)} Test Loss: {total_loss / len(train_loader)}, '\n",
    "                  f'Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df1be032-2699-4e69-a658-cd9dfba41086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.070943194277146 Test Loss: 1.17000785353137, Train Accuracy: 15.95%, Test Accuracy: 16.29%\n",
      "Epoch 11: Train Loss: 2.058769822412846 Test Loss: 1.1643859992424648, Train Accuracy: 17.81%, Test Accuracy: 17.46%\n",
      "Epoch 21: Train Loss: 2.0513469223298277 Test Loss: 1.1558385435857026, Train Accuracy: 18.24%, Test Accuracy: 18.25%\n",
      "Epoch 31: Train Loss: 2.028886291326261 Test Loss: 1.1464399245439791, Train Accuracy: 21.66%, Test Accuracy: 25.15%\n",
      "Epoch 41: Train Loss: 1.9783088106150721 Test Loss: 1.1289796563340169, Train Accuracy: 25.99%, Test Accuracy: 25.33%\n",
      "Epoch 51: Train Loss: 1.8723801827313853 Test Loss: 1.048267317753212, Train Accuracy: 32.17%, Test Accuracy: 33.41%\n",
      "Epoch 61: Train Loss: 1.6890615145949757 Test Loss: 0.9478972773341572, Train Accuracy: 42.04%, Test Accuracy: 42.75%\n",
      "Epoch 71: Train Loss: 1.4216347925511061 Test Loss: 0.7820450428361986, Train Accuracy: 54.26%, Test Accuracy: 54.74%\n",
      "Epoch 81: Train Loss: 1.173432003487559 Test Loss: 0.6788297959548586, Train Accuracy: 59.64%, Test Accuracy: 59.34%\n",
      "Epoch 91: Train Loss: 0.8846309235429063 Test Loss: 0.5418143563118636, Train Accuracy: 77.37%, Test Accuracy: 74.11%\n",
      "Epoch 101: Train Loss: 0.7495065842042951 Test Loss: 0.4611710463456499, Train Accuracy: 83.17%, Test Accuracy: 80.28%\n",
      "Epoch 111: Train Loss: 0.7394798199335734 Test Loss: 0.45856498643429494, Train Accuracy: 83.93%, Test Accuracy: 81.32%\n",
      "Epoch 121: Train Loss: 0.26381262687637524 Test Loss: 0.08954817969945497, Train Accuracy: 91.94%, Test Accuracy: 95.13%\n",
      "Epoch 131: Train Loss: 0.04839196475650704 Test Loss: 0.024453554516167417, Train Accuracy: 98.74%, Test Accuracy: 98.57%\n",
      "Epoch 141: Train Loss: 0.02175933291522704 Test Loss: 0.01006002194837635, Train Accuracy: 99.43%, Test Accuracy: 99.52%\n",
      "Epoch 151: Train Loss: 0.0007884485772485598 Test Loss: 0.005536404126208557, Train Accuracy: 100.00%, Test Accuracy: 99.70%\n",
      "Epoch 161: Train Loss: 0.00042559686424319094 Test Loss: 0.005576051571204195, Train Accuracy: 100.00%, Test Accuracy: 99.70%\n",
      "Epoch 171: Train Loss: 0.0003038432269914112 Test Loss: 0.005634410117721142, Train Accuracy: 100.00%, Test Accuracy: 99.74%\n",
      "Epoch 181: Train Loss: 0.00024177286360438802 Test Loss: 0.005724312913080925, Train Accuracy: 100.00%, Test Accuracy: 99.74%\n",
      "Epoch 191: Train Loss: 0.00020422851196367535 Test Loss: 0.00580194374904919, Train Accuracy: 100.00%, Test Accuracy: 99.74%\n",
      "Epoch 200: Train Loss: 0.00018116998160835733 Test Loss: 0.005788086610642031, Train Accuracy: 100.00%, Test Accuracy: 99.74%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# Assuming your SpeedPredictor and SpeedDataset are already defined and imported\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = SpeedDataset('outputs', sequence_length=25)\n",
    "test_size = int(0.15 * len(train_dataset))  # 10% for testing\n",
    "train_size = len(train_dataset) - test_size  # 90% for training\n",
    "\n",
    "# Split the dataset\n",
    "train_subset, test_subset = random_split(train_dataset, [train_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=10, shuffle=False)\n",
    "\n",
    "# Model setup\n",
    "max_classes = train_dataset.num_classes\n",
    "model = SpeedPredictor(sequence_length=25, feature_size=7, embedding_dim=64, hidden_dim=128, output_size=max_classes)\n",
    "model.to(device)  # Ensure model is on the appropriate device\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Ensure loss function is on the appropriate device\n",
    "optimizer = SGD(model.parameters(), lr=0.08)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.99)  # Learning rate scheduler\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14bb4e32-9e8d-4179-9224-6c65ca79665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix_and_report(all_targets, all_preds):\n",
    "    print(confusion_matrix(all_targets, all_preds))\n",
    "    print(classification_report(all_targets, all_preds, target_names=[f'Class {30 + i * 10}-{39 + i * 10}' for i in range(max(all_targets) + 1)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7bb827a-1680-4e24-96b0-6cca8631c76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix and Classification Report:\n",
      "[[447   0   0   0   0   0   0   0]\n",
      " [  1 424   0   0   0   0   0   0]\n",
      " [  0   0 375   0   0   0   0   0]\n",
      " [  1   0   0 356   2   0   0   0]\n",
      " [  0   0   0   0 293   1   0   0]\n",
      " [  0   0   0   0   0 140   0   0]\n",
      " [  0   0   0   0   0   0 160   0]\n",
      " [  0   0   1   0   0   0   0 101]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Class 30-39       1.00      1.00      1.00       447\n",
      "  Class 40-49       1.00      1.00      1.00       425\n",
      "  Class 50-59       1.00      1.00      1.00       375\n",
      "  Class 60-69       1.00      0.99      1.00       359\n",
      "  Class 70-79       0.99      1.00      0.99       294\n",
      "  Class 80-89       0.99      1.00      1.00       140\n",
      "  Class 90-99       1.00      1.00      1.00       160\n",
      "Class 100-109       1.00      0.99      1.00       102\n",
      "\n",
      "     accuracy                           1.00      2302\n",
      "    macro avg       1.00      1.00      1.00      2302\n",
      " weighted avg       1.00      1.00      1.00      2302\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.73935708079931"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Ensure inputs and targets are on the same device as model\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_test += (predicted == targets).sum().item()\n",
    "            total_test += targets.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_accuracy = 100 * correct_test / total_test\n",
    "    print(\"Confusion Matrix and Classification Report:\")\n",
    "    print_confusion_matrix_and_report(all_targets, all_preds)\n",
    "    return test_accuracy\n",
    "evaluate(model, test_loader)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
